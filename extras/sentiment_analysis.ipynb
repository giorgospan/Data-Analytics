{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MgWqAxIpWas",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !wget -O \"/content/drive/My Drive/datasets.tar.gz\"  --user bigdata --password d@t@s3t  195.134.67.98/documents/BigData/datasets2020.tar.gz\n",
        "# !tar -xvzf \"/content/drive/My Drive/datasets\" -C \"/content/drive/My Drive/\"\n",
        "\n",
        "# !wget -O \"/content/drive/My Drive/glove.6B.zip\" http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip \"/content/drive/My Drive/glove.6B.zip\" -d \"/content/drive/My Drive/glove\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J8_t4jE4iHG",
        "colab_type": "code",
        "outputId": "c11579a5-40d4-4cbb-bb91-7d553f6864df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "from keras.preprocessing.text import one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Conv1D, MaxPooling1D\n",
        "\n",
        "K.tensorflow_backend._get_available_gpus()\n",
        "\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S47_RACo7NH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "with open('/content/drive/My Drive/extra_stopwords.txt') as f:\n",
        "    for line in f:\n",
        "        stop_words.add(line[:-1])\n",
        "stop_words = list(stop_words)\n",
        "\n",
        "df_train = pd.read_csv('/content/drive/My Drive/datasets/q3/train.csv', encoding='utf-8')\n",
        "df_train['Content'] = df_train['Content'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n",
        "df_train['Content'] = df_train['Content'].apply(word_tokenize)\n",
        "df_train['Content'] = df_train['Content'].apply(lambda x:[word for word in x if word not in (stop_words) and len(word)>1])\n",
        "reviews  = df_train['Content'] \n",
        "labels = df_train['Label']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AguCY0phZJuB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare tokenizer\n",
        "t = Tokenizer(oov_token=True)\n",
        "t.fit_on_texts(reviews)\n",
        "vocab_size = len(t.word_index) + 1\n",
        "\n",
        "# integer encode the reviews\n",
        "encoded_revs = t.texts_to_sequences(reviews)\n",
        "\n",
        "# pad the sequences to maxlen\n",
        "maxlen = 100\n",
        "encoded_revs = pad_sequences(encoded_revs,maxlen=maxlen)\n",
        "encoded_revs.shape\n",
        "\n",
        "# load the whole embedding into memory\n",
        "embeddings_index = dict()\n",
        "with open('/content/drive/My Drive/glove/glove.6B.100d.txt','r') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "\n",
        "# create a weight matrix for words in training docs\n",
        "embedding_matrix = np.zeros((vocab_size, 100))\n",
        "for word, i in t.word_index.items():\n",
        "\tembedding_vector = embeddings_index.get(word)\n",
        "\tif embedding_vector is not None:\n",
        "\t\tembedding_matrix[i] = embedding_vector"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XdJSWxz8g8Lx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Embedding\n",
        "max_features = 20000\n",
        "maxlen = 100\n",
        "embedding_size = 100\n",
        "\n",
        "# Convolution\n",
        "kernel_size = 5\n",
        "filters = 64\n",
        "pool_size = 4\n",
        "\n",
        "# LSTM\n",
        "lstm_output_size = 70\n",
        "\n",
        "# Training (only 2 epochs are needed as the dataset is very small)\n",
        "batch_size = 30\n",
        "epochs = 2\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lHVqQUUwpxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "  # construct a CNN model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(vocab_size, embedding_size, weights=[embedding_matrix], input_length=maxlen))\n",
        "  # model.add(Embedding(vocab_size, embedding_size, input_length=maxlen))\n",
        "  model.add(Dropout(0.25))\n",
        "  model.add(Conv1D(filters,\n",
        "                  kernel_size,\n",
        "                  padding='valid',\n",
        "                  activation='relu',\n",
        "                  strides=1))\n",
        "  model.add(MaxPooling1D(pool_size=pool_size))\n",
        "  model.add(LSTM(lstm_output_size))\n",
        "  model.add(Dense(1))\n",
        "  model.add(Activation('sigmoid'))\n",
        "  # compile the model\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "  # print(model.summary())\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKq8qlCaVYkW",
        "colab_type": "code",
        "outputId": "c8234b25-30d3-4fd4-84f9-0df106c5df5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        }
      },
      "source": [
        "# Metrics have been removed from Keras core. We need to calculate them using sklearn.\n",
        "print('===============')\n",
        "print('Starting 5-fold')\n",
        "print('===============')\n",
        "\n",
        "kf = KFold(n_splits=5)\n",
        "accuracy = 0\n",
        "precision = 0\n",
        "recall = 0\n",
        "fmeasure = 0\n",
        "\n",
        "for train_index, test_index in kf.split(encoded_revs):\n",
        "  \n",
        "  # Fetch train and test data\n",
        "  X_train, X_test = encoded_revs[train_index], encoded_revs[test_index]\n",
        "  y_train, y_test = labels[train_index], labels[test_index]\n",
        "\n",
        "  # Create model\n",
        "  model = None\n",
        "  model = create_model()\n",
        "\n",
        "  # Fit on the train data\n",
        "  model.fit(X_train, y_train,batch_size=batch_size,epochs=epochs,validation_data=(X_test,y_test))\n",
        "\n",
        "  # Make predictions for test data\n",
        "  predictions  = model.predict_classes(X_test)\n",
        "  predictions  = [item for sublist in predictions for item in sublist]\n",
        "  \n",
        "  # Calculate metrics\n",
        "  accuracy += accuracy_score(y_test, predictions)\n",
        "  precision += precision_score(y_test, predictions, average='macro')\n",
        "  recall += recall_score(y_test, predictions, average='macro')\n",
        "  fmeasure += f1_score(y_test, predictions, average='macro')\n",
        "\n",
        "accuracy /= 5\n",
        "precision /= 5\n",
        "recall /= 5\n",
        "fmeasure /= 5\n",
        "\n",
        "print('accuracy = {}, precision = {}, recall = {}, f1-measure = {}'.format(round(accuracy, 4), round(precision,4), round(recall,4), round(fmeasure,4)))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "===============\n",
            "Starting 5-fold\n",
            "===============\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 41s 2ms/step - loss: 0.4500 - acc: 0.7867 - val_loss: 0.3207 - val_acc: 0.8642\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 38s 2ms/step - loss: 0.2536 - acc: 0.8988 - val_loss: 0.2874 - val_acc: 0.8834\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 41s 2ms/step - loss: 0.4423 - acc: 0.7931 - val_loss: 0.3283 - val_acc: 0.8612\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 38s 2ms/step - loss: 0.2497 - acc: 0.8994 - val_loss: 0.2986 - val_acc: 0.8734\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 41s 2ms/step - loss: 0.4414 - acc: 0.7924 - val_loss: 0.3681 - val_acc: 0.8370\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 37s 2ms/step - loss: 0.2479 - acc: 0.8999 - val_loss: 0.3204 - val_acc: 0.8712\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 42s 2ms/step - loss: 0.4446 - acc: 0.7880 - val_loss: 0.3166 - val_acc: 0.8632\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.2506 - acc: 0.8986 - val_loss: 0.2986 - val_acc: 0.8800\n",
            "Train on 20000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "20000/20000 [==============================] - 45s 2ms/step - loss: 0.4451 - acc: 0.7864 - val_loss: 0.3249 - val_acc: 0.8590\n",
            "Epoch 2/2\n",
            "20000/20000 [==============================] - 40s 2ms/step - loss: 0.2528 - acc: 0.8978 - val_loss: 0.2863 - val_acc: 0.8806\n",
            "accuracy = 0.8777, precision = 0.8781, recall = 0.8777, f1-measure = 0.8777\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxEUOoi1n1WI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test  = pd.read_csv('/content/drive/My Drive/datasets/q3/test_without_labels.csv', encoding='utf-8')\n",
        "df_test['Content'] = df_test['Content'].str.encode('ascii', 'ignore').str.decode('ascii').str.lower().str.replace('<br />','')\n",
        "test_reviews  = df_test['Content']\n",
        "\n",
        "encoded_test = t.texts_to_sequences(test_reviews)\n",
        "encoded_test = pad_sequences(encoded_test,maxlen=maxlen)\n",
        "\n",
        "predictions  = model.predict_classes(encoded_test)\n",
        "predictions  = [item for sublist in predictions for item in sublist]\n",
        "\n",
        "result = pd.DataFrame({'Id':df_test['Id'],'Predicted':predictions})\n",
        "result.to_csv('sentiment_predictions_keras.csv', sep=',', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "VMJZR1rDVX0Z",
        "colab": {}
      },
      "source": [
        "# history = model.fit(encoded_revs, labels, epochs=5)\n",
        "# plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "# plt.title('model accuracy')\n",
        "# plt.ylabel('accuracy')\n",
        "# plt.xlabel('epoch')\n",
        "# plt.legend(['train', 'test'], loc='upper left')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}